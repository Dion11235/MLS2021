{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size='8'>  Assignment 1 </font></center> \n",
    "<center><b><font size='4'>-Dipan Banik </font></b></center>\n",
    "\n",
    "\n",
    "- <font size='5'> **Introduction :** </font> <br>One approach to solve Linear Regression Problem on a data set $D\\{x_i,y_i\\}_{1}^{m}$ is $\\textbf{Probabilistic Modelling}$. In probabilistic models, we introduce a concept of random noise ($\\varepsilon_i$) which measures the unknown error effects. \n",
    "\n",
    "$$y_i = \\theta^TX_i + \\varepsilon_i \\quad \\textrm{where} \\quad \\varepsilon_i\\sim N(0,\\sigma^2)$$\n",
    "\n",
    "[**Note:**: $\\varepsilon_i$'s are independent and identically distributed as the observations are independent of each other]\n",
    "\\begin{align*}\n",
    "    p(\\varepsilon_i) &= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\frac{-\\varepsilon_i^2}{2\\sigma^2}} \\\\\n",
    "    \\implies p(y_i-\\theta^TX_i) &= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\frac{-(y_i-\\theta^Tx_i)^2}{2\\sigma^2}} \\\\\n",
    "\\end{align*}\n",
    "However the conventional way to write the probability is, \n",
    "$$p(y_i|x_i ; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\frac{-(y_i-\\theta^Tx_i)^2}{2\\sigma^2}}$$\n",
    "\n",
    "- <font size='5'> **Parameter Estimation :** </font><br>\n",
    "The regression coefficients $\\theta$ are generally estimated using Maximum Likelihood Estimation. In this process we try to estimate the parameters of the population by the observed data. $L(\\theta|D)$ represents the likelihood of the parameter taking that value where $D$ represents the data, and that likelihood is exactly same as the probability of getting that data when $\\theta$ is the population parameter. So we shall find a vector $\\theta$ such that $P(D|\\theta)$ is maximized.\n",
    "\n",
    "<br /><font size='5'> Estimating $\\theta$ with MLE (Maximum Likelihood Estimator): </font>\n",
    "\n",
    "**Theorem :**\n",
    "*Under Gaussian assumption of the Noise in the prediction, Linear Regression amounts to least square i.e. ordinary least square.*\n",
    "\n",
    "**Proof:**\n",
    "\\begin{align*}\n",
    "    \\theta^* &= argmax_{\\theta} L(\\theta|D) \\\\\n",
    "    &= argmax_{\\theta} P(D|\\theta) \\\\\n",
    "    &= argmax_{\\theta} P(y_1,x_1,y_2,x_2,\\dots,y_m,x_m;\\theta) \\\\\n",
    "    &= argmax_{\\theta} \\prod_{i=1}^m P(y_i,x_i;\\theta) &&  \\text{[As, the data observations are independent]} \\\\\n",
    "    &= argmax_{\\theta} \\prod_{i=1}^m P(y_i|x_i;\\theta)P(x_i|\\theta) \\\\\n",
    "    &= argmax_{\\theta} \\prod_{i=1}^m P(y_i|x_i;\\theta) && \\text{[As $x_i$'s are independent of $\\theta$]} \\\\\n",
    "    &= argmax_{\\theta} \\sum_{i=1}^m [\\log(\\frac{1}{\\sqrt{2\\pi}\\sigma})+\\log(\\exp(\\frac{-(y_i-\\theta^Tx_i)^2}{2\\sigma^2}))] && \\text{[As log is monotone increasing function]} \\\\\n",
    "    &= argmax_{\\theta} -\\frac{1}{2\\sigma^2}\\sum_{i=1}^m ((y_i-\\theta^Tx_i)^2) \\\\\n",
    "    &= argmin_{\\theta} \\frac{1}{m}\\sum_{i=1}^m (y_i-\\theta^Tx_i)^2 \\\\\n",
    "\\end{align*}\n",
    "$\\dots$ $\\text{which is nothing but the least square method.}$\n",
    "So, **Probabilistic Modelling under Gaussian assumption of noise is ultimately equivalent to Least square regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
