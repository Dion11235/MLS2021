# -*- coding: utf-8 -*-
"""CNN_assignment_unfinished.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17OV9StQlDZmXNhVtL3dweQ5P0AS-gwwF
"""

import torch
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import cv2
import csv
import imageio
from PIL import Image
from torch.utils.data.sampler import SubsetRandomSampler

"""## Data Preprocessing"""

## Step1 : importing the data from git
!git clone https://github.com/YoongiKim/CIFAR-10-images

## Step2 : Activating CUDA on Google Colab
train_on_gpu = torch.cuda.is_available()
if train_on_gpu:
  print("CUDA is available. Training on GPU...")
else:
  print("CUDA is not available. Cant train on GPU...")

## Step3 : Defining Data Transformations
def do_your_transform(X):
  train_transforms = transforms.Compose([transforms.RandomRotation(30),
                                         transforms.RandomHorizontalFlip(),
                                         transforms.ToTensor(),
                                         transforms.Normalize([0.5, 0.5, 0.5], 
                                                              [0.5, 0.5, 0.5])])

  X1 = train_transforms(X)
  return X1

## Step4 : Writting a custom Dataloader
class MyDataset1():
  
  def __init__(self,image_set,labels_reference,argument=True):
    df = pd.read_csv(image_set)
    self.imgfiles=list(df.iloc[:,0])
    self.classlabels=list(df.iloc[:,1])
    self.argument=argument
    self.ref=labels_reference 

  def __len__(self):
    return len(self.imgfiles)

  def __getitem__(self,idx):
    img=cv2.imread(self.imgfiles[idx])
    #X=np.asarray(img,dtype=np.)
    #print(X.shape)
    if self.argument:
      img = Image.fromarray(img)
      img=do_your_transform(img)
    Y=self.ref[self.classlabels[idx]]
    return img,Y

labels_dict = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4 , 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}

## Making the csv to read all the files
pathtrain = "CIFAR-10-images/train/"
pathtest = "CIFAR-10-images/test/"

files_in_train = os.listdir(pathtrain)
files_in_test = os.listdir(pathtest)

train = []; test = []
labels1 = []; labels2 = []

df_train = pd.DataFrame()
df_test = pd.DataFrame()

for folder in files_in_train:
  path = pathtrain + str(folder)
  for image in os.listdir(path):
    train.append(path + '/' + str(image))
    labels1.append(str(folder))

for folder in files_in_test:
  path = pathtest + str(folder)
  for image in os.listdir(path):
    test.append(path + '/' + str(image))
    labels2.append(str(folder))

df_train['path'] = train
df_train['classes'] = labels1

df_test['path'] = test
df_test['classes'] = labels2

df_train.to_csv('train_data.csv',index=False)
df_test.to_csv('test_data.csv',index=False)

## Preparing the iterators for dataloading
train_data=MyDataset1('train_data.csv',labels_dict)
train_data1 = MyDataset1("train_data.csv",labels_dict,argument=False)
test_data=MyDataset1('test_data.csv',labels_dict, argument=False)

batch_size=20
valid_size=0.2

# obtain training indices that will be used for validation
num_train = len(train_data)
indices = list(range(num_train))
np.random.shuffle(indices)
split = int(np.floor(valid_size * num_train))
train_idx, valid_idx = indices[split:], indices[:split]

# define samplers for obtaining training and validation batches
train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)

trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)
trainloader1 = torch.utils.data.DataLoader(train_data1, batch_size=batch_size)
validloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)
testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)

# ## Preparing the iterators for dataloading
# train_data = MyDataset1("train_data.csv",labels_dict)
# train_data1 = MyDataset1("train_data.csv",labels_dict,argument=False)
# test_data = MyDataset1("test_data.csv",labels_dict,argument=False)

# trainloader = torch.utils.data.DataLoader(train_data, batch_size = 64)
# trainloader1 = torch.utils.data.DataLoader(train_data1, batch_size = 64)
# testloader = torch.utils.data.DataLoader(test_data, batch_size = 64, shuffle = True)

a = torch.tensor(2)
print(a.numpy())

## Step5 : visualizing a batch of train and test data 
def myimshow(image, label, ref, ax=None, title=None, normalize=True):
    """Imshow for Tensor."""
    if ax is None:
        fig, ax = plt.subplots()
    print("before : ", image.shape)
    if image.shape[2] != 3:
      image = image.numpy().transpose((1, 2, 0))
    else:
      image = image.numpy()
    print("after : ", image.shape)

    if normalize:
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image = std * image + mean
        image = np.clip(image, 0, 1)

    label = list(ref.keys())[label.numpy()]

    ax.imshow(image)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.tick_params(axis='both', length=0)
    ax.set_xticklabels('')
    ax.set_yticklabels('')
    ax.set_title(label)

    return ax

images, labels = next(iter(trainloader1))
#print(images.shape)

fig, axes = plt.subplots(figsize=(10,4), ncols=4)
fig.suptitle("Train Data (Before Augmentation and transformation) :")
fig.tight_layout()
for ii in range(4):
    ax = axes[ii]
    myimshow(images[ii], ax=ax,label=labels[ii],ref=labels_dict,normalize=False)

images, labels = next(iter(trainloader))

fig, axes = plt.subplots(figsize=(10,4), ncols=4)
fig.suptitle("Train Data (After Augmentation and transformation):")
fig.tight_layout()
for ii in range(4):
    ax = axes[ii]
    myimshow(images[ii], ax=ax,label=labels[ii],ref=labels_dict, normalize=False)

images, labels = next(iter(testloader))

fig, axes = plt.subplots(figsize=(10,4), ncols=4)
fig.suptitle("Test Data :")
fig.tight_layout()
for ii in range(4):
    ax = axes[ii]
    myimshow(images[ii], ax=ax,label=labels[ii],ref=labels_dict, normalize=False)

"""## Model Training"""

## Step6 : Building the model
import torch.nn as nn
import torch.nn.functional as F

# define the CNN architecture
class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        # convolutional layer (sees 32x32x3 image tensor)
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        # convolutional layer (sees 16x16x16 tensor)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        # convolutional layer (sees 8x8x32 tensor)
        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
        # max pooling layer
        self.pool = nn.MaxPool2d(2, 2)
        # linear layer (64 * 4 * 4 -> 500)
        self.fc1 = nn.Linear(64 * 4 * 4, 500)
        # linear layer (500 -> 10)
        self.fc2 = nn.Linear(500, 10)
        # dropout layer (p=0.25)
        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        # add sequence of convolutional and max pooling layers
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        # flatten image input
        x = x.view(-1, 64 * 4 * 4)
        # add dropout layer
        x = self.dropout(x)
        # add 1st hidden layer, with relu activation function
        x = F.relu(self.fc1(x))
        # add dropout layer
        x = self.dropout(x)
        # add 2nd hidden layer, with relu activation function
        x = self.fc2(x)
        return x

import torch.optim as optim

# specify loss function (categorical cross-entropy)
model = ConvNet()
criterion = nn.CrossEntropyLoss()

# specify optimizer
optimizer = optim.SGD(model.parameters(), lr=0.01)

if train_on_gpu:
    model.cuda()

print(model)

# number of epochs to train the model
n_epochs = 30

valid_loss_min = np.Inf # track change in validation loss

for epoch in range(1, n_epochs+1):

    # keep track of training and validation loss
    train_loss = 0.0
    valid_loss = 0.0
    
    ###################
    # train the model #
    ###################
    model.train()
    for batch_idx, (data, target) in enumerate(trainloader):
        # move tensors to GPU if CUDA is available
        if train_on_gpu:
            data, target = data.cuda(), target.cuda()
        # clear the gradients of all optimized variables
        optimizer.zero_grad()
        #print(data.shape)
        # forward pass: compute predicted outputs by passing inputs to the model
        output = model(data)
        #print(output.shape)
        top_p,top_class=output.topk(1,dim=1)
        equalstry=top_class==target
        equals=top_class==target.view(*top_class.shape)
        accuracy=torch.mean(equals.type(torch.FloatTensor))


        print(f'Accuracy: {accuracy.item()*100}%')

        # calculate the batch loss
        loss = criterion(output, target)
        # backward pass: compute gradient of the loss with respect to model parameters
        loss.backward()
        # perform a single optimization step (parameter update)
        optimizer.step()
        # update training loss
        train_loss += loss.item()*data.size(0)
        
    ######################    
    # validate the model #
    ######################
    model.eval()
    for batch_idx, (data, target) in enumerate(validloader):
        # move tensors to GPU if CUDA is available
        if train_on_gpu:
            data, target = data.cuda(), target.cuda()
        # forward pass: compute predicted outputs by passing inputs to the model
        output = model(data)
        print(output.shape)
        # calculate the batch loss
        loss = criterion(output, target)
        # update average validation loss 
        valid_loss += loss.item()*data.size(0)
    
    # calculate average losses
    train_loss = train_loss/len(trainloader.sampler)
    valid_loss = valid_loss/len(validloader.sampler)
        
    # print training/validation statistics 
    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
        epoch, train_loss, valid_loss))
    
    # save model if validation loss has decreased
    if valid_loss <= valid_loss_min:
        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(
        valid_loss_min,
        valid_loss))
        torch.save(model.state_dict(), 'model_augmented.pt')
        valid_loss_min = valid_loss

